{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression & Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import plot_learning_curve\n",
    "from scipy.signal import welch\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "\n",
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PCA features\n",
    "As performed in the previous step, we shall be using our PCA features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3810\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|    0 |    1 |    2 |    3 |    4 |    5 |    6 |    7 |    8 |    9 |   10 |   11 |   12 |   13 |   14 |\n",
      "|------+------+------+------+------+------+------+------+------+------+------+------+------+------+------|\n",
      "| 2.80 | 0.14 | 0.53 | 0.55 | 0.22 | 0.96 | 0.11 | 0.81 | 0.31 | 0.07 | 1.08 | 0.53 | 2.22 | 1.06 | 0.22 |\n",
      "| 0.95 | 0.35 | 0.67 | 0.31 | 0.30 | 0.04 | 0.27 | 0.23 | 0.44 | 0.08 | 0.64 | 0.79 | 0.40 | 0.52 | 0.11 |\n",
      "| 2.66 | 0.14 | 0.74 | 0.44 | 0.54 | 0.18 | 0.63 | 0.12 | 0.35 | 0.47 | 0.30 | 0.26 | 0.43 | 0.98 | 0.07 |\n",
      "| 1.32 | 0.50 | 1.40 | 1.13 | 4.44 | 0.52 | 1.16 | 0.88 | 0.77 | 1.51 | 0.48 | 0.57 | 0.34 | 0.43 | 0.12 |\n",
      "| 3.56 | 0.02 | 1.25 | 0.59 | 0.11 | 1.53 | 0.11 | 0.29 | 0.27 | 0.27 | 0.40 | 0.19 | 1.02 | 0.34 | 0.23 |\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n"
     ]
    }
   ],
   "source": [
    "# Import PCA features and labels\n",
    "X = np.loadtxt('./data/PCA_features.csv',delimiter=',')\n",
    "print(X.shape[0])\n",
    "print(tabulate(X[0:5],floatfmt = \".2f\",headers = [(i) for i in range(15)],tablefmt='psql',numalign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode y-labels\n",
    "We do not have to convert into one-hot encoding as the tools we use directly operates on discrete class integers. However, we still need to encode from strings into class integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['concrete', 'soft_pvc', 'wood', 'tiled', 'fine_concrete', 'hard_tiles_large_space', 'soft_tiles', 'carpet', 'hard_tiles']\n",
      "[4. 0. 0. ... 4. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.read_csv('./data/y_train.csv')\n",
    "y_encoded_train = np.zeros(y_train.shape[0])\n",
    "y_labels = list(y_train['surface'].value_counts().index)\n",
    "i = 0\n",
    "for surface in y_labels:\n",
    "    y_encoded_train[y_train['surface']==surface] = i\n",
    "    i += 1\n",
    "print(y_labels)\n",
    "print(y_encoded_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise features\n",
    "We use z-scoring to normalise our features distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "[[2.79885583 0.14018706 0.53344835 0.54947368 0.22398822]\n",
      " [0.95024313 0.3537402  0.66782446 0.31184005 0.30197932]\n",
      " [2.66298508 0.14100292 0.7353114  0.44199593 0.54238961]\n",
      " [1.31882634 0.50290392 1.40031595 1.13372449 4.43853231]\n",
      " [3.5550391  0.02075805 1.24775028 0.58754519 0.11419715]]\n",
      "After scaling:\n",
      "[[-0.18836596 -0.10746136 -0.40703808 -0.34302846 -0.47355155]\n",
      " [-0.56514982 -0.02605698 -0.34516169 -0.4999286  -0.42202239]\n",
      " [-0.2160591  -0.10715036 -0.31408587 -0.41399179 -0.26318194]\n",
      " [-0.49002527  0.03080278 -0.00787015  0.04272936  2.31102166]\n",
      " [-0.03424083 -0.15298654 -0.07812231 -0.31789134 -0.54609113]]\n"
     ]
    }
   ],
   "source": [
    "print('Before scaling:')\n",
    "print(X[0:5,0:5])\n",
    "zscore = StandardScaler()\n",
    "X = zscore.fit_transform(X) #scales each feature column\n",
    "print('After scaling:')\n",
    "print(X[0:5,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train-test sets\n",
    "We will split our PCA features/labels into train and test sets, where the test set will be used as a final evaluation later. Train will be further split into validation and train sets during K-Fold Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get score of model used to classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X_train, X_val, Y_train,Y_val):\n",
    "    model.fit(X_train,Y_train)\n",
    "    return model.score(X_val,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine each set of hyperparameters for K-fold evaluation\n",
    "In our models, we shall attempt to find the best of hyperparameters. The tunable hyperparameters are:\n",
    "\n",
    "Log Reg\n",
    "- Regularisation parameter C\n",
    "\n",
    "SVM\n",
    "- Regularisation parameter C\n",
    "- Kernel type: 'rbf', 'poly' and 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = [0.01,0.03,0.1,0.3,1,3,10]\n",
    "kernels = ['rbf','poly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Log Reg: Perform Stratified K-Fold Split (k= 5) for each model permutation\n",
    "Stratified K-Fold is used to ensure that there is a uniform distribution of surfaces in all train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Regularisation = 0.01\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.17877176018545132\n",
      "Model Regularisation = 0.03\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.17975981481437134\n",
      "Model Regularisation = 0.1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.1830504167771713\n",
      "Model Regularisation = 0.3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18435761329989492\n",
      "Model Regularisation = 1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.1843554773464587\n",
      "Model Regularisation = 3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18337294225591977\n",
      "Model Regularisation = 10\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18304183778578134\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "val_loss_all = {}\n",
    "\n",
    "for reg in regs:\n",
    "    print('Model Regularisation = {}'.format(reg))\n",
    "    score_logreg = []\n",
    "\n",
    "    kf = StratifiedKFold(n_splits)\n",
    "\n",
    "    for i,(train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "        print(\"Training on fold \" + str(i+1) + \"/{}...\".format(n_splits))\n",
    "        score_logreg.append(get_score(LogisticRegression(C=reg,multi_class='ovr',solver='lbfgs'), X_train[train_index,:],X_train[val_index,:], y_encoded_train[train_index], y_encoded_train[val_index]))\n",
    "    val_loss_all[reg] = np.mean(score_logreg)\n",
    "    print('Average Validation Loss:{}'.format(np.mean(score_logreg)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best combinaton belongs to C=0.01, with a lowest val loss of 0.17877176018545132.\n"
     ]
    }
   ],
   "source": [
    "# Get best hyperparameter set based on val loss\n",
    "val_loss_best,best_hyper = min(val_loss_all.values()),min(val_loss_all,key=val_loss_all.get)\n",
    "print('The best combinaton belongs to C={}, with a lowest val loss of {}.'.format(best_hyper,val_loss_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model with best set of hyperparameter & evaluate final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy score: 0.42782152230971127\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=best_hyper,multi_class='ovr',solver='lbfgs')\n",
    "model.fit(X_train,y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "score = accuracy_score(predictions,y_test)\n",
    "print('Final accuracy score:',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 9 classes to predict on, 42-44% is already better than baseline accuracy obtained via random guessing, i.e. 1/9 chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) SVM: Perform Stratified K-Fold Split (k= 5) for each model permutation\n",
    "Similar to above Log Reg model, we train on different combinations of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Kernel = rbf, Regularisation = 0.01\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.17025884364356073\n",
      "Model Kernel = rbf, Regularisation = 0.03\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.17025884364356073\n",
      "Model Kernel = rbf, Regularisation = 0.1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.1777762968611198\n",
      "Model Kernel = rbf, Regularisation = 0.3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.17645515675613058\n",
      "Model Kernel = rbf, Regularisation = 1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.17843000442198204\n",
      "Model Kernel = rbf, Regularisation = 3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.17778833006918995\n",
      "Model Kernel = rbf, Regularisation = 10\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.17024714880435174\n",
      "Model Kernel = poly, Regularisation = 0.01\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.1666446095575291\n",
      "Model Kernel = poly, Regularisation = 0.03\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.1669671068388725\n",
      "Model Kernel = poly, Regularisation = 0.1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16598564321517167\n",
      "Model Kernel = poly, Regularisation = 0.3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16368940518890512\n",
      "Model Kernel = poly, Regularisation = 1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16270470594752487\n",
      "Model Kernel = poly, Regularisation = 3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16008491313355686\n",
      "Model Kernel = poly, Regularisation = 10\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.15677920454919797\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "val_loss_all = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    for reg in regs:\n",
    "        print('Model Kernel = {}, Regularisation = {}'.format(kernel,reg))\n",
    "        score_svc = []\n",
    "\n",
    "        kf = StratifiedKFold(n_splits)\n",
    "\n",
    "        for i,(train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "            print(\"Training on fold \" + str(i+1) + \"/{}...\".format(n_splits))\n",
    "            score_svc.append(get_score(SVC(C=reg,kernel=kernel,gamma='auto',degree=6), X_train[train_index,:],X_train[val_index,:], y_encoded_train[train_index],y_encoded_train[val_index]))\n",
    "        val_loss_all[(kernel,reg)] = np.mean(score_svc)\n",
    "        print('Average Validation Loss:{}'.format(np.mean(score_svc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best combinaton belongs to ('poly', 10), with a lowest val loss of 0.15677920454919797.\n"
     ]
    }
   ],
   "source": [
    "# Get best hyperparameter set based on val loss\n",
    "val_loss_best,best_hyper = min(val_loss_all.values()),min(val_loss_all,key=val_loss_all.get)\n",
    "print('The best combinaton belongs to {}, with a lowest val loss of {}.'.format(best_hyper,val_loss_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model with best set of hyperparameter & evaluate final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-286e3ac17719>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_hyper\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_hyper\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model = SVC(C=best_hyper[1],kernel=best_hyper[0],gamma='auto')\n",
    "model.fit(X_train,y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "score = accuracy_score(predictions,y_test)\n",
    "print('Final accuracy score:',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of test results\n",
    "Logistic Regression with regularisation of 0.01 achieved an accuracy of 43% while SVC with polynomial features and regularisation of 10 gave an accuracy of 48%. This is expected as the polynomial has a degree of 6 which is able to better represent any non-linear patterns in the data. Furthermore, we see that it performed best with the highest possible regularisation value of 10, indicating that it has avoided high variance problem and generalised better than other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
