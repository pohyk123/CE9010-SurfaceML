{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression & Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "\n",
    "np.random.seed(41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PCA features\n",
    "As performed in the previous step, we shall be using our PCA features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3810\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|    0 |    1 |    2 |    3 |    4 |    5 |    6 |    7 |    8 |    9 |   10 |   11 |   12 |   13 |   14 |\n",
      "|------+------+------+------+------+------+------+------+------+------+------+------+------+------+------|\n",
      "| 2.80 | 0.14 | 0.53 | 0.55 | 0.22 | 0.96 | 0.11 | 0.81 | 0.31 | 0.07 | 1.08 | 0.53 | 2.22 | 1.06 | 0.22 |\n",
      "| 0.95 | 0.35 | 0.67 | 0.31 | 0.30 | 0.04 | 0.27 | 0.23 | 0.44 | 0.08 | 0.64 | 0.79 | 0.40 | 0.52 | 0.11 |\n",
      "| 2.66 | 0.14 | 0.74 | 0.44 | 0.54 | 0.18 | 0.63 | 0.12 | 0.35 | 0.47 | 0.30 | 0.26 | 0.43 | 0.98 | 0.07 |\n",
      "| 1.32 | 0.50 | 1.40 | 1.13 | 4.44 | 0.52 | 1.16 | 0.88 | 0.77 | 1.51 | 0.48 | 0.57 | 0.34 | 0.43 | 0.12 |\n",
      "| 3.56 | 0.02 | 1.25 | 0.59 | 0.11 | 1.53 | 0.11 | 0.29 | 0.27 | 0.27 | 0.40 | 0.19 | 1.02 | 0.34 | 0.23 |\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n"
     ]
    }
   ],
   "source": [
    "# Import PCA features and labels\n",
    "X = np.loadtxt('./data/PCA_features.csv',delimiter=',')\n",
    "print(X.shape[0])\n",
    "print(tabulate(X[0:5],floatfmt = \".2f\",headers = [(i) for i in range(15)],tablefmt='psql',numalign='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode y-labels\n",
    "We do not have to convert into one-hot encoding as the tools we use directly operates on discrete class integers. However, we still need to encode from strings into class integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['concrete', 'soft_pvc', 'wood', 'tiled', 'fine_concrete', 'hard_tiles_large_space', 'soft_tiles', 'carpet', 'hard_tiles']\n",
      "[4. 0. 0. ... 4. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_train = pd.read_csv('./data/y_train.csv')\n",
    "y_encoded_train = np.zeros(y_train.shape[0])\n",
    "y_labels = list(y_train['surface'].value_counts().index)\n",
    "i = 0\n",
    "for surface in y_labels:\n",
    "    y_encoded_train[y_train['surface']==surface] = i\n",
    "    i += 1\n",
    "print(y_labels)\n",
    "print(y_encoded_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise features\n",
    "We use z-scoring to normalise our features distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      "[[2.79885583 0.14018706 0.53344835 0.54947368 0.22398822]\n",
      " [0.95024313 0.3537402  0.66782446 0.31184005 0.30197932]\n",
      " [2.66298508 0.14100292 0.7353114  0.44199593 0.54238961]\n",
      " [1.31882634 0.50290392 1.40031595 1.13372449 4.43853231]\n",
      " [3.5550391  0.02075805 1.24775028 0.58754519 0.11419715]]\n",
      "After scaling:\n",
      "[[-0.18836596 -0.10746136 -0.40703808 -0.34302846 -0.47355155]\n",
      " [-0.56514982 -0.02605698 -0.34516169 -0.4999286  -0.42202239]\n",
      " [-0.2160591  -0.10715036 -0.31408587 -0.41399179 -0.26318194]\n",
      " [-0.49002527  0.03080278 -0.00787015  0.04272936  2.31102166]\n",
      " [-0.03424083 -0.15298654 -0.07812231 -0.31789134 -0.54609113]]\n"
     ]
    }
   ],
   "source": [
    "print('Before scaling:')\n",
    "print(X[0:5,0:5])\n",
    "zscore = StandardScaler()\n",
    "X = zscore.fit_transform(X) #scales each feature column\n",
    "print('After scaling:')\n",
    "print(X[0:5,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train-test sets\n",
    "We will split our PCA features/labels into train and test sets, where the test set will be used as a final evaluation later. Train will be further split into validation and train sets during K-Fold Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get score of model used to classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X_train, X_val, Y_train,Y_val):\n",
    "    model.fit(X_train,Y_train)\n",
    "    return model.score(X_val,Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine each set of hyperparameters for K-fold evaluation\n",
    "In our models, we shall attempt to find the best of hyperparameters. The tunable hyperparameters are:\n",
    "\n",
    "Log Reg\n",
    "- Regularisation parameter C\n",
    "\n",
    "SVM\n",
    "- Regularisation parameter C\n",
    "- Kernel type: 'rbf', 'poly' and 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = [0.01,0.03,0.1,0.3,1,3,10]\n",
    "kernels = ['rbf','poly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Log Reg: Perform Stratified K-Fold Split (k= 5) for each model permutation\n",
    "Stratified K-Fold is used to ensure that there is a uniform distribution of surfaces in all train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Regularisation = 0.01\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18175045796928485\n",
      "Model Regularisation = 0.03\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18535650337367882\n",
      "Model Regularisation = 0.1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18108718816265565\n",
      "Model Regularisation = 0.3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18108450151761957\n",
      "Model Regularisation = 1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.1810812765000835\n",
      "Model Regularisation = 3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18141076579168153\n",
      "Model Regularisation = 10\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18141238623082054\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "val_loss_all = {}\n",
    "\n",
    "for reg in regs:\n",
    "    print('Model Regularisation = {}'.format(reg))\n",
    "    score_logreg = []\n",
    "\n",
    "    kf = StratifiedKFold(n_splits)\n",
    "\n",
    "    for i,(train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "        print(\"Training on fold \" + str(i+1) + \"/{}...\".format(n_splits))\n",
    "        score_logreg.append(get_score(LogisticRegression(C=reg,multi_class='ovr',solver='lbfgs'), X_train[train_index,:],X_train[val_index,:], y_encoded_train[train_index], y_encoded_train[val_index]))\n",
    "    val_loss_all[reg] = np.mean(score_logreg)\n",
    "    print('Average Validation Loss:{}'.format(np.mean(score_logreg)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best combinaton belongs to C=1, with a lowest val loss of 0.1810812765000835.\n"
     ]
    }
   ],
   "source": [
    "# Get best hyperparameter set based on val loss\n",
    "val_loss_best,best_hyper = min(val_loss_all.values()),min(val_loss_all,key=val_loss_all.get)\n",
    "print('The best combinaton belongs to C={}, with a lowest val loss of {}.'.format(best_hyper,val_loss_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model with best set of hyperparameter & evaluate final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy score: 0.45013123359580054\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=best_hyper,multi_class='ovr',solver='lbfgs')\n",
    "model.fit(X_train,y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "score = accuracy_score(predictions,y_test)\n",
    "print('Final accuracy score:',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 9 classes to predict on, 42-44% is already better than baseline accuracy obtained via random guessing, i.e. 1/9 chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) SVM: Perform Stratified K-Fold Split (k= 5) for each model permutation\n",
    "Similar to above Log Reg model, we train on different combinations of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Kernel = rbf, Regularisation = 0.01\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16929454502908162\n",
      "Model Kernel = rbf, Regularisation = 0.03\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16929454502908162\n",
      "Model Kernel = rbf, Regularisation = 0.1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16929454502908162\n",
      "Model Kernel = rbf, Regularisation = 0.3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.1761530250706267\n",
      "Model Kernel = rbf, Regularisation = 1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.19224126349415646\n",
      "Model Kernel = rbf, Regularisation = 3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.18930761393322137\n",
      "Model Kernel = rbf, Regularisation = 10\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.19226706885190828\n",
      "Model Kernel = poly, Regularisation = 0.01\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16896183071994755\n",
      "Model Kernel = poly, Regularisation = 0.03\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16699567327198778\n",
      "Model Kernel = poly, Regularisation = 0.1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16437215235833175\n",
      "Model Kernel = poly, Regularisation = 0.3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16600565904430103\n",
      "Model Kernel = poly, Regularisation = 1\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16338210640916107\n",
      "Model Kernel = poly, Regularisation = 3\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16599866531508983\n",
      "Model Kernel = poly, Regularisation = 10\n",
      "Training on fold 1/5...\n",
      "Training on fold 2/5...\n",
      "Training on fold 3/5...\n",
      "Training on fold 4/5...\n",
      "Training on fold 5/5...\n",
      "Average Validation Loss:0.16370783752818666\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "val_loss_all = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    for reg in regs:\n",
    "        print('Model Kernel = {}, Regularisation = {}'.format(kernel,reg))\n",
    "        score_svc = []\n",
    "\n",
    "        kf = StratifiedKFold(n_splits)\n",
    "\n",
    "        for i,(train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "            print(\"Training on fold \" + str(i+1) + \"/{}...\".format(n_splits))\n",
    "            score_svc.append(get_score(SVC(C=reg,kernel=kernel,gamma='auto',degree=6), X_train[train_index,:],X_train[val_index,:], y_encoded_train[train_index],y_encoded_train[val_index]))\n",
    "        val_loss_all[(kernel,reg)] = np.mean(score_svc)\n",
    "        print('Average Validation Loss:{}'.format(np.mean(score_svc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best combinaton belongs to ('poly', 1), with a lowest val loss of 0.16338210640916107.\n"
     ]
    }
   ],
   "source": [
    "# Get best hyperparameter set based on val loss\n",
    "val_loss_best,best_hyper = min(val_loss_all.values()),min(val_loss_all,key=val_loss_all.get)\n",
    "print('The best combinaton belongs to {}, with a lowest val loss of {}.'.format(best_hyper,val_loss_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model with best set of hyperparameter & evaluate final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy score: 0.3766404199475066\n"
     ]
    }
   ],
   "source": [
    "model = SVC(C=best_hyper[1],kernel=best_hyper[0],gamma='auto')\n",
    "model.fit(X_train,y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "score = accuracy_score(predictions,y_test)\n",
    "print('Final accuracy score:',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
